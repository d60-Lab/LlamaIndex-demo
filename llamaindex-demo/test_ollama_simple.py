"""
ç®€å•çš„ Ollama æµ‹è¯•
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent / "src"))

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

def test_simple():
    print("ğŸ§ª ç®€å• Ollama æµ‹è¯•")
    
    try:
        # æµ‹è¯• LLM
        llm = Ollama(model="deepseek-r1", request_timeout=120.0)
        response = llm.complete("ä½ å¥½ï¼")
        print(f"âœ… LLM æµ‹è¯•æˆåŠŸ: {response.text[:50]}...")
        
        # æµ‹è¯•åµŒå…¥
        embed_model = OllamaEmbedding(
            model_name="nomic-embed-text",
        )
        embedding = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
        print(f"âœ… åµŒå…¥æµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        
        # æµ‹è¯•å®Œæ•´çš„ RAG æµç¨‹
        print("\nğŸ“š æµ‹è¯•å®Œæ•´ RAG æµç¨‹...")
        documents = SimpleDirectoryReader("./data").load_data()
        print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        # ä½¿ç”¨è¾ƒå°çš„ chunk é¿å…åµŒå…¥é—®é¢˜
        from llama_index.core.node_parser import SentenceSplitter
        parser = SentenceSplitter(chunk_size=256, chunk_overlap=20)
        nodes = parser.get_nodes_from_documents(documents[:1])  # åªç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£æµ‹è¯•
        
        print(f"åˆ†å‰²ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        # æ„å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            documents=documents[:1],  # åªç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£
            embed_model=embed_model
        )
        
        # æŸ¥è¯¢
        query_engine = index.as_query_engine(llm=llm)
        response = query_engine.query("LlamaIndex æ˜¯ä»€ä¹ˆï¼Ÿ")
        
        print(f"âœ… RAG æµ‹è¯•æˆåŠŸ!")
        print(f"å›ç­”: {response.response[:100]}...")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_simple()
