"""
ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
åŸºäº LlamaIndex æ„å»ºçš„ä¼ä¸šçº§æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
"""

import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
from dotenv import load_dotenv

from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Document
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.vector_stores import MetadataFilters, FilterCondition

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœæ•°æ®ç±»"""
    answer: str
    sources: List[Dict]
    confidence: float
    query_time: float

class EnterpriseKnowledgeBase:
    """ä¼ä¸šçŸ¥è¯†åº“ç±»"""
    
    def __init__(self, 
                 persist_dir: str = "./storage",
                 embed_model: str = "text-embedding-3-small",
                 llm_model: str = "gpt-3.5-turbo"):
        
        self.persist_dir = persist_dir
        self.embed_model = OpenAIEmbedding(model=embed_model)
        self.llm = OpenAI(model=llm_model)
        self.index = None
        self.query_engine = None
        
    def initialize(self, data_dir: str = "./data", force_rebuild: bool = False):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        print("ğŸš€ åˆå§‹åŒ–ä¼ä¸šçŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
        if force_rebuild or not os.path.exists(self.persist_dir):
            print("ğŸ“š æ„å»ºæ–°ç´¢å¼•...")
            self._build_index(data_dir)
        else:
            print("ğŸ“‚ åŠ è½½å·²æœ‰ç´¢å¼•...")
            self._load_index()
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self._create_query_engine()
        print("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _build_index(self, data_dir: str):
        """æ„å»ºæ–°çš„ç´¢å¼•"""
        # åŠ è½½æ–‡æ¡£
        documents = SimpleDirectoryReader(
            data_dir,
            recursive=True,
            required_exts=[".md", ".pdf", ".txt", ".docx"]
        ).load_data()
        
        if not documents:
            raise ValueError(f"åœ¨ç›®å½• {data_dir} ä¸­æœªæ‰¾åˆ°æ–‡æ¡£")
        
        # å¢å¼ºæ–‡æ¡£å…ƒæ•°æ®
        documents = self._enhance_documents(documents)
        
        # æ–‡æ¡£åˆ†å—
        parser = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50,
            paragraph_separator="\n\n"
        )
        nodes = parser.get_nodes_from_documents(documents)
        
        # æ„å»ºç´¢å¼•
        self.index = VectorStoreIndex(
            nodes=nodes,
            embed_model=self.embed_model
        )
        
        # æŒä¹…åŒ–ç´¢å¼•
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ° {self.persist_dir}")
    
    def _load_index(self):
        """åŠ è½½å·²æœ‰ç´¢å¼•"""
        storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
        self.index = load_index_from_storage(storage_context)
    
    def _enhance_documents(self, documents: List[Document]) -> List[Document]:
        """å¢å¼ºæ–‡æ¡£å…ƒæ•°æ®"""
        enhanced_docs = []
        
        for doc in documents:
            # æå–æ–‡ä»¶ä¿¡æ¯
            file_path = doc.metadata.get('file_path', '')
            file_name = os.path.basename(file_path)
            file_ext = os.path.splitext(file_name)[1]
            
            # å¢å¼ºå…ƒæ•°æ®
            enhanced_metadata = {
                **doc.metadata,
                'file_name': file_name,
                'file_type': file_ext,
                'doc_size': len(doc.text),
                'department': self._infer_department(file_name, doc.text),
                'category': self._infer_category(file_name, doc.text),
                'indexed_at': str(pd.Timestamp.now())
            }
            
            enhanced_doc = Document(
                text=doc.text,
                metadata=enhanced_metadata,
                id_=f"doc_{hash(doc.text)}"
            )
            enhanced_docs.append(enhanced_doc)
        
        return enhanced_docs
    
    def _infer_department(self, file_name: str, content: str) -> str:
        """æ¨æ–­æ–‡æ¡£æ‰€å±éƒ¨é—¨"""
        content_lower = content.lower()
        file_name_lower = file_name.lower()
        
        departments = {
            'engineering': ['æŠ€æœ¯', 'å¼€å‘', 'å·¥ç¨‹', 'ä»£ç ', 'api', 'ç³»ç»Ÿ'],
            'sales': ['é”€å”®', 'å®¢æˆ·', 'åˆåŒ', 'è®¢å•', 'ä¸šç»©'],
            'marketing': ['å¸‚åœº', 'è¥é”€', 'æ¨å¹¿', 'å“ç‰Œ', 'æ´»åŠ¨'],
            'hr': ['äººäº‹', 'æ‹›è˜', 'å‘˜å·¥', 'åŸ¹è®­', 'è–ªé…¬'],
            'finance': ['è´¢åŠ¡', 'é¢„ç®—', 'æˆæœ¬', 'æ”¶å…¥', 'æŠ¥è¡¨'],
            'product': ['äº§å“', 'éœ€æ±‚', 'åŠŸèƒ½', 'è®¾è®¡', 'ç”¨æˆ·']
        }
        
        for dept, keywords in departments.items():
            if any(keyword in content_lower or keyword in file_name_lower 
                   for keyword in keywords):
                return dept
        
        return 'general'
    
    def _infer_category(self, file_name: str, content: str) -> str:
        """æ¨æ–­æ–‡æ¡£ç±»åˆ«"""
        content_lower = content.lower()
        
        categories = {
            'tutorial': ['æ•™ç¨‹', 'æŒ‡å—', 'å…¥é—¨', 'å¦‚ä½•', 'æ­¥éª¤'],
            'documentation': ['æ–‡æ¡£', 'è¯´æ˜', 'å‚è€ƒ', 'æ‰‹å†Œ'],
            'policy': ['æ”¿ç­–', 'è§„å®š', 'åˆ¶åº¦', 'æµç¨‹'],
            'report': ['æŠ¥å‘Š', 'æ€»ç»“', 'åˆ†æ', 'ç»Ÿè®¡'],
            'meeting': ['ä¼šè®®', 'çºªè¦', 'è®¨è®º', 'å†³ç­–']
        }
        
        for category, keywords in categories.items():
            if any(keyword in content_lower for keyword in keywords):
                return category
        
        return 'general'
    
    def _create_query_engine(self):
        """åˆ›å»ºæŸ¥è¯¢å¼•æ“"""
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=5
        )
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[
                SimilarityPostprocessor(similarity_cutoff=0.7)
            ]
        )
    
    def query(self, 
              question: str, 
              filters: Optional[Dict] = None,
              top_k: int = 3) -> QueryResult:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        import time
        start_time = time.time()
        
        try:
            # åº”ç”¨è¿‡æ»¤å™¨
            if filters:
                metadata_filters = MetadataFilters(
                    filters=[
                        {"key": k, "value": v} 
                        for k, v in filters.items()
                    ],
                    condition=FilterCondition.AND
                )
                self.query_engine = self.index.as_query_engine(
                    filters=metadata_filters,
                    similarity_top_k=top_k
                )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = self.query_engine.query(question)
            
            # æå–æ¥æºä¿¡æ¯
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes[:top_k]:
                    sources.append({
                        'file_name': node.metadata.get('file_name', 'æœªçŸ¥'),
                        'department': node.metadata.get('department', 'æœªçŸ¥'),
                        'category': node.metadata.get('category', 'æœªçŸ¥'),
                        'relevance': getattr(node, 'score', 0),
                        'snippet': node.text[:200] + "..." if len(node.text) > 200 else node.text
                    })
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(response, sources)
            
            query_time = time.time() - start_time
            
            return QueryResult(
                answer=response.response,
                sources=sources,
                confidence=confidence,
                query_time=query_time
            )
            
        except Exception as e:
            return QueryResult(
                answer=f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                sources=[],
                confidence=0.0,
                query_time=time.time() - start_time
            )
    
    def _calculate_confidence(self, response, sources: List[Dict]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not sources:
            return 0.0
        
        # åŸºäºæ¥æºç›¸å…³åº¦è®¡ç®—ç½®ä¿¡åº¦
        relevance_scores = [src['relevance'] for src in sources if src['relevance'] > 0]
        if not relevance_scores:
            return 0.0
        
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        return min(avg_relevance, 1.0)
    
    def add_documents(self, file_paths: List[str]):
        """æ·»åŠ æ–°æ–‡æ¡£"""
        print(f"ğŸ“„ æ·»åŠ  {len(file_paths)} ä¸ªæ–°æ–‡æ¡£...")
        
        # åŠ è½½æ–°æ–‡æ¡£
        new_documents = SimpleDirectoryReader(
            input_files=file_paths
        ).load_data()
        
        # å¢å¼ºå…ƒæ•°æ®
        new_documents = self._enhance_documents(new_documents)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        for doc in new_documents:
            self.index.insert(doc)
        
        # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print("âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
    
    def get_statistics(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–ç´¢å¼•ä¸­çš„èŠ‚ç‚¹æ•°é‡
            docstore = self.index.docstore
            node_count = len(docstore.docs)
            
            # è·å–å­˜å‚¨å¤§å°
            storage_size = 0
            if os.path.exists(self.persist_dir):
                for root, dirs, files in os.walk(self.persist_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if os.path.exists(file_path):
                            storage_size += os.path.getsize(file_path)
            
            return {
                'total_documents': node_count,
                'storage_size_mb': round(storage_size / (1024 * 1024), 2),
                'storage_path': self.persist_dir,
                'embedding_model': self.embed_model.model_name,
                'llm_model': self.llm.model
            }
            
        except Exception as e:
            return {'error': str(e)}

def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("ğŸ¢ ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    kb = EnterpriseKnowledgeBase()
    
    try:
        kb.initialize()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = kb.get_statistics()
        print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
        print(f"  æ–‡æ¡£æ•°é‡: {stats.get('total_documents', 'æœªçŸ¥')}")
        print(f"  å­˜å‚¨å¤§å°: {stats.get('storage_size_mb', 'æœªçŸ¥')} MB")
        print(f"  åµŒå…¥æ¨¡å‹: {stats.get('embedding_model', 'æœªçŸ¥')}")
        
        print("\nğŸš€ ç³»ç»Ÿå·²å°±ç»ªï¼å¼€å§‹æé—®å§ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
        print("ğŸ’¡ æç¤º: ä½ å¯ä»¥ä½¿ç”¨è¿‡æ»¤å™¨ï¼Œå¦‚ 'department:engineering æŠ€æœ¯é—®é¢˜'")
        print("=" * 60)
        
        while True:
            try:
                user_input = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # è§£æè¿‡æ»¤å™¨
                filters = {}
                question = user_input
                
                if ':' in user_input:
                    parts = user_input.split(':', 1)
                    if len(parts) == 2 and len(parts[0].split()) == 1:
                        filter_key = parts[0].strip()
                        filter_value = parts[1].split()[0].strip()
                        question = ' '.join(parts[1].split()[1:])
                        filters[filter_key] = filter_value
                
                print("ğŸ¤” æ€è€ƒä¸­...")
                result = kb.query(question, filters=filters)
                
                print(f"\nğŸ’¡ å›ç­”:")
                print(result.answer)
                
                print(f"\nğŸ“Š ç½®ä¿¡åº¦: {result.confidence:.2f} | â±ï¸ ç”¨æ—¶: {result.query_time:.2f}s")
                
                if result.sources:
                    print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                    for i, source in enumerate(result.sources, 1):
                        print(f"  {i}. {source['file_name']} "
                              f"({source['department']} | {source['category']}) "
                              f"[ç›¸å…³åº¦: {source['relevance']:.2f}]")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‡ºç°é”™è¯¯: {e}")
    
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    # éœ€è¦å®‰è£… pandas ç”¨äºæ—¶é—´æˆ³
    try:
        import pandas as pd
    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£… pandas: pip install pandas")
        exit(1)
    
    interactive_demo()