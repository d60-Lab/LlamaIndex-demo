"""
LLM å·¥å‚ç±»
æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼šOllamaã€DeepSeekã€OpenAIã€Poe ç­‰
"""

import os
from typing import Optional
from dotenv import load_dotenv
from llama_index.llms.openai import OpenAI
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.embeddings.ollama import OllamaEmbedding

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMFactory:
    """LLM å·¥å‚ç±»ï¼Œæ ¹æ®é…ç½®åˆ›å»ºä¸åŒçš„ LLM å®ä¾‹"""
    
    @staticmethod
    def get_llm(provider: Optional[str] = None, model: Optional[str] = None):
        """è·å– LLM å®ä¾‹"""
        provider = provider or os.getenv("LLM_PROVIDER", "ollama")
        
        if provider.lower() == "ollama":
            return LLMFactory._create_ollama_llm(model)
        elif provider.lower() == "deepseek":
            return LLMFactory._create_deepseek_llm(model)
        elif provider.lower() == "openai":
            return LLMFactory._create_openai_llm(model)
        elif provider.lower() == "poe":
            return LLMFactory._create_poe_llm(model)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM æä¾›å•†: {provider}")
    
    @staticmethod
    def get_embedding_model(provider: Optional[str] = None, model: Optional[str] = None):
        """è·å–åµŒå…¥æ¨¡å‹å®ä¾‹"""
        provider = provider or os.getenv("EMBEDDING_PROVIDER", "ollama")
        
        if provider.lower() == "ollama":
            return LLMFactory._create_ollama_embedding(model)
        elif provider.lower() == "openai":
            return LLMFactory._create_openai_embedding(model)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥æ¨¡å‹æä¾›å•†: {provider}")
    
    @staticmethod
    def _create_ollama_llm(model: Optional[str] = None):
        """åˆ›å»º Ollama LLM"""
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        model_name = model or os.getenv("OLLAMA_MODEL", "deepseek-r1")
        
        try:
            return Ollama(
                model=model_name,
                base_url=base_url,
                temperature=0.1,
                request_timeout=120.0
            )
        except Exception as e:
            raise ValueError(f"æ— æ³•è¿æ¥åˆ° Ollama: {e}\nè¯·ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œåœ¨ {base_url}")
    
    @staticmethod
    def _create_ollama_embedding(model: Optional[str] = None):
        """åˆ›å»º Ollama åµŒå…¥æ¨¡å‹"""
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        model_name = model or os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
        
        try:
            return OllamaEmbedding(
                model_name=model_name,
                base_url=base_url
            )
        except Exception as e:
            raise ValueError(f"æ— æ³•è¿æ¥åˆ° Ollama åµŒå…¥æ¨¡å‹: {e}\nè¯·ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œåœ¨ {base_url}")
    
    @staticmethod
    def _create_poe_llm(model: Optional[str] = None):
        """åˆ›å»º Poe LLM"""
        api_key = os.getenv("POE_API_KEY")
        base_url = os.getenv("POE_BASE_URL", "https://api.poe.com")
        
        if not api_key:
            raise ValueError("è¯·è®¾ç½® POE_API_KEY ç¯å¢ƒå˜é‡")
        
        model_name = model or os.getenv("POE_MODEL", "claude-3-haiku-20240307")
        
        # Poe ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        return OpenAI(
            model=model_name,
            api_key=api_key,
            api_base=base_url,
            temperature=0.1,
            max_tokens=4096
        )
    
    @staticmethod
    def _create_deepseek_llm(model: Optional[str] = None):
        """åˆ›å»º DeepSeek LLM"""
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        
        if not api_key:
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        
        # DeepSeek æ¨¡å‹æ˜ å°„
        model_mapping = {
            "fast": "deepseek-chat",
            "reasoning": "deepseek-reasoner",
            "code": "deepseek-coder"
        }
        
        if model and model in model_mapping:
            model_name = model_mapping[model]
        else:
            model_name = model or "deepseek-chat"
        
        return OpenAI(
            model=model_name,
            api_key=api_key,
            api_base=base_url,
            temperature=0.1,
            max_tokens=4096
        )
    
    @staticmethod
    def _create_openai_llm(model: Optional[str] = None):
        """åˆ›å»º OpenAI LLM"""
        api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        model_name = model or "gpt-3.5-turbo"
        
        return OpenAI(
            model=model_name,
            api_key=api_key,
            temperature=0.1,
            max_tokens=4096
        )
    
    @staticmethod
    def _create_openai_embedding(model: Optional[str] = None):
        """åˆ›å»º OpenAI åµŒå…¥æ¨¡å‹"""
        api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            raise ValueError("åµŒå…¥æ¨¡å‹éœ€è¦è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        model_name = model or "text-embedding-3-small"
        
        return OpenAIEmbedding(
            model=model_name,
            api_key=api_key
        )
    
    @staticmethod
    def get_available_providers():
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        providers = []
        
        # æ£€æŸ¥ Ollamaï¼ˆæœ¬åœ°æœåŠ¡ï¼‰
        try:
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                providers.append("ollama")
        except:
            pass
        
        if os.getenv("DEEPSEEK_API_KEY"):
            providers.append("deepseek")
        
        if os.getenv("OPENAI_API_KEY"):
            providers.append("openai")
        
        if os.getenv("POE_API_KEY"):
            providers.append("poe")
        
        return providers
    
    @staticmethod
    def get_provider_info():
        """è·å–æä¾›å•†ä¿¡æ¯"""
        info = {
            "available_providers": LLMFactory.get_available_providers(),
            "current_llm_provider": os.getenv("LLM_PROVIDER", "ollama"),
            "current_embedding_provider": os.getenv("EMBEDDING_PROVIDER", "ollama"),
            "models": {
                "ollama": {
                    "chat": "deepseek-r1",
                    "embedding": "nomic-embed-text"
                },
                "deepseek": {
                    "chat": "deepseek-chat",
                    "reasoner": "deepseek-reasoner", 
                    "coder": "deepseek-coder"
                },
                "openai": {
                    "chat": "gpt-3.5-turbo",
                    "advanced": "gpt-4-turbo",
                    "embedding": "text-embedding-3-small"
                },
                "poe": {
                    "chat": "claude-3-haiku-20240307",
                    "advanced": "claude-3-sonnet-20240229"
                }
            }
        }
        return info

def test_llm_connection():
    """æµ‹è¯• LLM è¿æ¥"""
    print("ğŸ§ª æµ‹è¯• LLM è¿æ¥")
    print("=" * 40)
    
    try:
        # è·å–æä¾›å•†ä¿¡æ¯
        info = LLMFactory.get_provider_info()
        print(f"ğŸ“‹ å¯ç”¨æä¾›å•†: {info['available_providers']}")
        print(f"ğŸ¯ å½“å‰ LLM æä¾›å•†: {info['current_llm_provider']}")
        print(f"ğŸ”¤ å½“å‰åµŒå…¥æä¾›å•†: {info['current_embedding_provider']}")
        
        # æµ‹è¯• LLM
        llm = LLMFactory.get_llm()
        print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ: {llm.model}")
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        response = llm.complete("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
        print(f"ğŸ’¬ æµ‹è¯•å¯¹è¯: {response.text[:100]}...")
        
        # æµ‹è¯•åµŒå…¥æ¨¡å‹
        embedding = LLMFactory.get_embedding_model()
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {embedding.model_name}")
        
        # æµ‹è¯•åµŒå…¥
        text_embedding = embedding.get_text_embedding("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬")
        print(f"ğŸ”¢ åµŒå…¥ç»´åº¦: {len(text_embedding)}")
        
        print("\nğŸ‰ æ‰€æœ‰è¿æ¥æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    test_llm_connection()