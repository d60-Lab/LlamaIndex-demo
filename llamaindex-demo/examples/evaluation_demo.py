"""
è¯„ä¼°æ¡†æ¶ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LlamaIndex çš„è¯„ä¼°å·¥å…·æ¥è¡¡é‡ RAG ç³»ç»Ÿæ€§èƒ½
"""

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator,
    BatchEvalRunner
)
from llama_index.llms.openai import OpenAI
import pandas as pd
import asyncio
from typing import List, Dict
import json

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, query_engine):
        self.query_engine = query_engine
        self.llm = OpenAI(model="gpt-4-turbo")
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.faithfulness_evaluator = FaithfulnessEvaluator(llm=self.llm)
        self.relevancy_evaluator = RelevancyEvaluator(llm=self.llm)
        self.correctness_evaluator = CorrectnessEvaluator(llm=self.llm)
        self.semantic_similarity_evaluator = SemanticSimilarityEvaluator(llm=self.llm)
    
    def evaluate_single_query(self, query: str, reference_answer: str = None):
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        print(f"ğŸ” è¯„ä¼°æŸ¥è¯¢: {query}")
        
        # è·å–ç³»ç»Ÿå›ç­”
        response = self.query_engine.query(query)
        
        # è¯„ä¼°ç»“æœ
        results = {
            "query": query,
            "system_response": response.response,
            "reference_answer": reference_answer
        }
        
        try:
            # å¿ å®åº¦è¯„ä¼°ï¼ˆå›ç­”æ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼‰
            faith_result = self.faithfulness_evaluator.evaluate_response(response=response)
            results["faithfulness_score"] = faith_result.score
            results["faithfulness_feedback"] = faith_result.feedback
            
        except Exception as e:
            print(f"âŒ å¿ å®åº¦è¯„ä¼°å¤±è´¥: {e}")
            results["faithfulness_score"] = 0
            results["faithfulness_feedback"] = "è¯„ä¼°å¤±è´¥"
        
        try:
            # ç›¸å…³æ€§è¯„ä¼°ï¼ˆå›ç­”æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³ï¼‰
            rel_result = self.relevancy_evaluator.evaluate_response(
                query=query, 
                response=response
            )
            results["relevancy_score"] = rel_result.score
            results["relevancy_feedback"] = rel_result.feedback
            
        except Exception as e:
            print(f"âŒ ç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {e}")
            results["relevancy_score"] = 0
            results["relevancy_feedback"] = "è¯„ä¼°å¤±è´¥"
        
        try:
            # æ­£ç¡®æ€§è¯„ä¼°ï¼ˆå¦‚æœæœ‰å‚è€ƒç­”æ¡ˆï¼‰
            if reference_answer:
                corr_result = self.correctness_evaluator.evaluate_response(
                    query=query,
                    response=response,
                    reference=reference_answer
                )
                results["correctness_score"] = corr_result.score
                results["correctness_feedback"] = corr_result.feedback
            else:
                results["correctness_score"] = None
                results["correctness_feedback"] = "æ— å‚è€ƒç­”æ¡ˆ"
                
        except Exception as e:
            print(f"âŒ æ­£ç¡®æ€§è¯„ä¼°å¤±è´¥: {e}")
            results["correctness_score"] = 0
            results["correctness_feedback"] = "è¯„ä¼°å¤±è´¥"
        
        try:
            # è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°ï¼ˆå¦‚æœæœ‰å‚è€ƒç­”æ¡ˆï¼‰
            if reference_answer:
                sem_result = self.semantic_similarity_evaluator.evaluate_response(
                    query=query,
                    response=response,
                    reference=reference_answer
                )
                results["semantic_similarity_score"] = sem_result.score
            else:
                results["semantic_similarity_score"] = None
                
        except Exception as e:
            print(f"âŒ è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°å¤±è´¥: {e}")
            results["semantic_similarity_score"] = 0
        
        return results
    
    async def batch_evaluate(self, test_dataset: List[Dict]):
        """æ‰¹é‡è¯„ä¼°æŸ¥è¯¢"""
        print(f"ğŸ“Š å¼€å§‹æ‰¹é‡è¯„ä¼° {len(test_dataset)} ä¸ªæŸ¥è¯¢...")
        
        # å‡†å¤‡æŸ¥è¯¢å’Œå‚è€ƒç­”æ¡ˆåˆ—è¡¨
        queries = [item["query"] for item in test_dataset]
        reference_answers = [item.get("reference_answer") for item in test_dataset]
        
        # åˆ›å»ºæ‰¹é‡è¯„ä¼°å™¨
        runner = BatchEvalRunner(
            {
                "faithfulness": self.faithfulness_evaluator,
                "relevancy": self.relevancy_evaluator,
                "correctness": self.correctness_evaluator,
                "semantic_similarity": self.semantic_similarity_evaluator
            },
            workers=4
        )
        
        # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
        eval_results = await runner.aevaluate_queries(
            query_engine=self.query_engine,
            queries=queries,
            reference=reference_answers
        )
        
        return eval_results
    
    def generate_report(self, eval_results):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“‹ è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        
        if isinstance(eval_results, dict) and "faithfulness" in eval_results:
            # æ‰¹é‡è¯„ä¼°ç»“æœ
            self._generate_batch_report(eval_results)
        else:
            # å•ä¸ªè¯„ä¼°ç»“æœ
            self._generate_single_report(eval_results)
    
    def _generate_single_report(self, result):
        """ç”Ÿæˆå•ä¸ªæŸ¥è¯¢çš„è¯„ä¼°æŠ¥å‘Š"""
        print(f"æŸ¥è¯¢: {result['query']}")
        print(f"ç³»ç»Ÿå›ç­”: {result['system_response']}")
        
        if result.get('reference_answer'):
            print(f"å‚è€ƒç­”æ¡ˆ: {result['reference_answer']}")
        
        print(f"\nğŸ“Š è¯„ä¼°åˆ†æ•°:")
        print(f"  å¿ å®åº¦: {result.get('faithfulness_score', 'N/A')}/1.0")
        print(f"  ç›¸å…³æ€§: {result.get('relevancy_score', 'N/A')}/1.0")
        print(f"  æ­£ç¡®æ€§: {result.get('correctness_score', 'N/A')}/1.0")
        print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {result.get('semantic_similarity_score', 'N/A')}/1.0")
        
        print(f"\nğŸ’¬ åé¦ˆ:")
        if result.get('faithfulness_feedback'):
            print(f"  å¿ å®åº¦: {result['faithfulness_feedback']}")
        if result.get('relevancy_feedback'):
            print(f"  ç›¸å…³æ€§: {result['relevancy_feedback']}")
        if result.get('correctness_feedback'):
            print(f"  æ­£ç¡®æ€§: {result['correctness_feedback']}")
    
    def _generate_batch_report(self, eval_results):
        """ç”Ÿæˆæ‰¹é‡è¯„ä¼°æŠ¥å‘Š"""
        # è®¡ç®—å¹³å‡åˆ†æ•°
        metrics = {}
        for metric_name, results in eval_results.items():
            scores = [r.score for r in results if hasattr(r, 'score')]
            if scores:
                metrics[metric_name] = {
                    "average": sum(scores) / len(scores),
                    "min": min(scores),
                    "max": max(scores),
                    "count": len(scores)
                }
        
        print("ğŸ“Š æ‰¹é‡è¯„ä¼°ç»Ÿè®¡:")
        for metric_name, stats in metrics.items():
            print(f"  {metric_name}:")
            print(f"    å¹³å‡åˆ†æ•°: {stats['average']:.3f}")
            print(f"    æœ€é«˜åˆ†æ•°: {stats['max']:.3f}")
            print(f"    æœ€ä½åˆ†æ•°: {stats['min']:.3f}")
            print(f"    è¯„ä¼°æ•°é‡: {stats['count']}")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for metric_name, results in eval_results.items():
            print(f"\n{metric_name} è¯¦ç»†ç»“æœ:")
            for i, result in enumerate(results):
                if hasattr(result, 'query') and hasattr(result, 'score'):
                    print(f"  {i+1}. æŸ¥è¯¢: {result.query[:50]}...")
                    print(f"     åˆ†æ•°: {result.score:.3f}")
                    if hasattr(result, 'feedback') and result.feedback:
                        print(f"     åé¦ˆ: {result.feedback[:100]}...")

def create_test_dataset():
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
    return [
        {
            "query": "LlamaIndexçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
            "reference_answer": "LlamaIndexçš„ä¸»è¦ç‰¹æ€§åŒ…æ‹¬å¼€ç®±å³ç”¨ã€ä¸°å¯Œé›†æˆã€ç”Ÿäº§å°±ç»ªã€æ™ºèƒ½ä»£ç†å’Œå¯æ‰©å±•æ€§ã€‚"
        },
        {
            "query": "å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½ï¼Ÿ",
            "reference_answer": "å¯ä»¥é€šè¿‡æ··åˆæ£€ç´¢ã€é‡æ’åºã€æŸ¥è¯¢å˜æ¢ã€ä¼˜åŒ–åˆ†å—ç­–ç•¥å’Œä½¿ç”¨ä¸“ä¸šå‘é‡æ•°æ®åº“æ¥ä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½ã€‚"
        },
        {
            "query": "LlamaIndexæ”¯æŒå“ªäº›ç±»å‹çš„ç´¢å¼•ï¼Ÿ",
            "reference_answer": "LlamaIndexæ”¯æŒå‘é‡ç´¢å¼•ã€æ ‘å½¢ç´¢å¼•ã€çŸ¥è¯†å›¾è°±ç´¢å¼•ã€åˆ—è¡¨ç´¢å¼•ç­‰å¤šç§ç´¢å¼•ç±»å‹ã€‚"
        },
        {
            "query": "ä»€ä¹ˆæ˜¯ReActä»£ç†ï¼Ÿ",
            "reference_answer": "ReActä»£ç†æ˜¯ä¸€ç§æ¨ç†-è¡ŒåŠ¨å¾ªç¯çš„ä»£ç†æ¨¡å¼ï¼Œé€‚åˆéœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ã€‚"
        },
        {
            "query": "å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½ï¼Ÿ",
            "reference_answer": "å¯ä»¥ä½¿ç”¨å¿ å®åº¦ã€ç›¸å…³æ€§ã€æ­£ç¡®æ€§ç­‰è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚"
        }
    ]

def create_query_engine():
    """åˆ›å»ºæŸ¥è¯¢å¼•æ“"""
    print("ğŸ—ï¸ åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“...")
    
    # åŠ è½½æ–‡æ¡£
    documents = SimpleDirectoryReader("./data").load_data()
    
    # åˆ›å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(documents)
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        llm=OpenAI(model="gpt-3.5-turbo")
    )
    
    print("âœ… æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    return query_engine

async def main():
    """è¯„ä¼°æ¼”ç¤ºä¸»å‡½æ•°"""
    print("ğŸ¯ LlamaIndex è¯„ä¼°æ¡†æ¶æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = create_query_engine()
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = RAGEvaluator(query_engine)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = create_test_dataset()
        
        print(f"\nğŸ“ æµ‹è¯•æ•°æ®é›†åŒ…å« {len(test_dataset)} ä¸ªæŸ¥è¯¢")
        
        # å•ä¸ªæŸ¥è¯¢è¯„ä¼°æ¼”ç¤º
        print("\nğŸ” å•ä¸ªæŸ¥è¯¢è¯„ä¼°æ¼”ç¤º")
        print("=" * 40)
        sample_query = test_dataset[0]
        single_result = evaluator.evaluate_single_query(
            query=sample_query["query"],
            reference_answer=sample_query["reference_answer"]
        )
        evaluator.generate_report(single_result)
        
        # æ‰¹é‡è¯„ä¼°æ¼”ç¤º
        print("\nğŸ“Š æ‰¹é‡è¯„ä¼°æ¼”ç¤º")
        print("=" * 40)
        batch_results = await evaluator.batch_evaluate(test_dataset)
        evaluator.generate_report(batch_results)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        report = {
            "evaluation_type": "batch",
            "total_queries": len(test_dataset),
            "timestamp": pd.Timestamp.now().isoformat(),
            "results": batch_results
        }
        
        with open("./evaluation_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: ./evaluation_report.json")
        print("\nğŸ‰ è¯„ä¼°æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())
