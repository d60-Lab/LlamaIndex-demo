"""
å¤šæ¨¡æ€å¤„ç†ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LlamaIndex å¤„ç†å›¾åƒå’Œæ–‡æœ¬æ•°æ®
"""

import os
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core.schema import ImageDocument
from llama_index.llms.openai import OpenAI
import base64
from PIL import Image
import io

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def create_sample_image():
    """åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å›¾è¡¨å›¾åƒï¼ˆæ¨¡æ‹Ÿé”€å”®æ•°æ®å›¾è¡¨ï¼‰"""
    try:
        from PIL import Image, ImageDraw, ImageFont
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸ±çŠ¶å›¾
        width, height = 400, 300
        img = Image.new('RGB', (width, height), color='white')
        draw = ImageDraw.Draw(img)
        
        # ç»˜åˆ¶åæ ‡è½´
        draw.line([(50, 250), (350, 250)], fill='black', width=2)  # Xè½´
        draw.line([(50, 50), (50, 250)], fill='black', width=2)   # Yè½´
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        data = [120, 135, 148, 162]  # Q1-Q4é”€å”®é¢
        colors = ['blue', 'green', 'red', 'orange']
        labels = ['Q1', 'Q2', 'Q3', 'Q4']
        
        for i, (value, color, label) in enumerate(zip(data, colors, labels)):
            x = 80 + i * 70
            bar_height = int(value * 1.2)  # ç¼©æ”¾å› å­
            draw.rectangle([x-20, 250-bar_height, x+20, 250], fill=color)
            draw.text((x-15, 260), label, fill='black')
            draw.text((x-15, 235-bar_height), str(value), fill='black')
        
        # æ·»åŠ æ ‡é¢˜
        draw.text((120, 20), "2024å¹´å­£åº¦é”€å”®é¢ï¼ˆä¸‡ç¾å…ƒï¼‰", fill='black')
        
        # ä¿å­˜å›¾åƒ
        img_path = "./data/sales_chart.png"
        img.save(img_path)
        print(f"âœ… åˆ›å»ºç¤ºä¾‹å›¾è¡¨: {img_path}")
        return img_path
        
    except ImportError:
        print("âŒ PIL åº“æœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºç¤ºä¾‹å›¾åƒ")
        return None

def analyze_image_with_text(image_path: str, question: str):
    """ä½¿ç”¨å¤šæ¨¡æ€LLMåˆ†æå›¾åƒ"""
    print(f"ğŸ” åˆ†æå›¾åƒ: {image_path}")
    print(f"â“ é—®é¢˜: {question}")
    
    try:
        # åŠ è½½å›¾åƒæ–‡æ¡£
        image_documents = SimpleDirectoryReader(
            input_files=[image_path]
        ).load_data()
        
        # åˆå§‹åŒ–å¤šæ¨¡æ€LLM
        multimodal_llm = OpenAIMultiModal(model="gpt-4-vision-preview")
        
        # åˆ†æå›¾åƒ
        response = multimodal_llm.complete(
            prompt=question,
            image_documents=image_documents
        )
        
        print(f"ğŸ’¡ åˆ†æç»“æœ:")
        print(response.text)
        return response.text
        
    except Exception as e:
        print(f"âŒ å›¾åƒåˆ†æå¤±è´¥: {e}")
        return None

def create_mixed_index():
    """åˆ›å»ºåŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„æ··åˆç´¢å¼•"""
    print("ğŸ“š åˆ›å»ºæ··åˆæ¨¡æ€ç´¢å¼•...")
    
    # åŠ è½½æ–‡æœ¬æ–‡æ¡£
    text_documents = SimpleDirectoryReader(
        input_dir="./data",
        required_exts=[".md"]
    ).load_data()
    
    # åŠ è½½å›¾åƒæ–‡æ¡£
    image_documents = []
    image_files = ["./data/sales_chart.png"]
    
    for img_file in image_files:
        if os.path.exists(img_file):
            img_doc = ImageDocument(image_path=img_file)
            # ä¸ºå›¾åƒæ·»åŠ æè¿°æ€§å…ƒæ•°æ®
            img_doc.metadata = {
                "file_type": "image",
                "description": "2024å¹´å­£åº¦é”€å”®é¢æŸ±çŠ¶å›¾",
                "content": "é”€å”®æ•°æ®å¯è§†åŒ–"
            }
            image_documents.append(img_doc)
    
    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    all_documents = text_documents + image_documents
    
    # åˆ›å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(all_documents)
    query_engine = index.as_query_engine(
        llm=OpenAI(model="gpt-4-turbo"),
        similarity_top_k=3
    )
    
    print(f"âœ… æ··åˆç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(text_documents)} ä¸ªæ–‡æœ¬æ–‡æ¡£å’Œ {len(image_documents)} ä¸ªå›¾åƒæ–‡æ¡£")
    return query_engine

def text_to_image_analysis():
    """æ–‡æœ¬åˆ°å›¾åƒçš„åˆ†æç¤ºä¾‹"""
    print("\nğŸ¨ æ–‡æœ¬åˆ°å›¾åƒåˆ†æç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹å›¾åƒ
    image_path = create_sample_image()
    if not image_path:
        print("âŒ æ— æ³•åˆ›å»ºç¤ºä¾‹å›¾åƒï¼Œè·³è¿‡å¤šæ¨¡æ€æ¼”ç¤º")
        return
    
    # åˆ†æå›¾åƒçš„ä¸åŒæ–¹é¢
    questions = [
        "è¿™å¼ å›¾è¡¨å±•ç¤ºäº†ä»€ä¹ˆæ•°æ®ï¼Ÿ",
        "å“ªä¸ªå­£åº¦çš„é”€å”®é¢æœ€é«˜ï¼Ÿ",
        "åˆ†æè¿™ä¸ªé”€å”®è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºé¢„æµ‹",
        "ç”¨ä¸­æ–‡æè¿°å›¾è¡¨çš„ä¸»è¦ä¿¡æ¯"
    ]
    
    for question in questions:
        print(f"\n" + "="*50)
        analyze_image_with_text(image_path, question)

def multimodal_qa_demo():
    """å¤šæ¨¡æ€é—®ç­”æ¼”ç¤º"""
    print("\nğŸ¤– å¤šæ¨¡æ€é—®ç­”æ¼”ç¤º")
    print("=" * 50)
    
    query_engine = create_mixed_index()
    
    test_questions = [
        "LlamaIndexçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½ï¼Ÿ",
        "é”€å”®æ•°æ®è¡¨ç°å¦‚ä½•ï¼Ÿ",
        "å¯¹æ¯”ä¸åŒå­£åº¦çš„é”€å”®è¡¨ç°"
    ]
    
    for question in test_questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        print("ğŸ¤” æ€è€ƒä¸­...")
        
        try:
            response = query_engine.query(question)
            print(f"ğŸ’¡ å›ç­”:")
            print(response.response)
            
            # æ˜¾ç¤ºæ¥æºä¿¡æ¯
            if hasattr(response, 'source_nodes') and response.source_nodes:
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, node in enumerate(response.source_nodes, 1):
                    file_type = node.metadata.get('file_type', 'text')
                    file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                    score = getattr(node, 'score', 0)
                    print(f"  {i}. {file_name} ({file_type}, ç›¸å…³åº¦: {score:.2f})")
                    
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

def main():
    """å¤šæ¨¡æ€å¤„ç†ä¸»å‡½æ•°"""
    print("ğŸŒŸ LlamaIndex å¤šæ¨¡æ€å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    try:
        # æ–‡æœ¬åˆ°å›¾åƒåˆ†æ
        text_to_image_analysis()
        
        # å¤šæ¨¡æ€é—®ç­”
        multimodal_qa_demo()
        
        print("\nğŸ‰ å¤šæ¨¡æ€æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main()