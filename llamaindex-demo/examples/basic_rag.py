"""
åŸºç¡€ RAG åº”ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LlamaIndex æ„å»ºç®€å•çš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
"""

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def main():
    """åŸºç¡€ RAG åº”ç”¨ä¸»å‡½æ•°"""
    
    # 1. æ•°æ®åŠ è½½
    print("ğŸ“ åŠ è½½æ–‡æ¡£æ•°æ®...")
    documents = SimpleDirectoryReader("./data").load_data()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. æ–‡æ¡£åˆ†å—
    print("ğŸ”§ å¤„ç†æ–‡æ¡£åˆ†å—...")
    parser = SentenceSplitter(
        chunk_size=512,        # token æ•°é‡
        chunk_overlap=50,      # é‡å éƒ¨åˆ†ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
        paragraph_separator="\n\n"
    )
    nodes = parser.get_nodes_from_documents(documents)
    print(f"âœ… æ–‡æ¡£è¢«åˆ†å‰²ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹")
    
    # 3. æ„å»ºç´¢å¼•
    print("ğŸ—ï¸ æ„å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex(
        nodes=nodes,
        embed_model=OpenAIEmbedding(model="text-embedding-3-small")
    )
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
    
    # 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“
    print("ğŸ” åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“...")
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        response_mode="compact",
        llm=OpenAI(model="gpt-3.5-turbo"),
    )
    
    # 5. äº¤äº’æŸ¥è¯¢
    print("\nğŸš€ RAG ç³»ç»Ÿå·²å°±ç»ªï¼å¼€å§‹æé—®å§ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
    print("=" * 60)
    
    while True:
        try:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not question:
                continue
                
            print("ğŸ¤” æ€è€ƒä¸­...")
            response = query_engine.query(question)
            
            print(f"\nğŸ’¡ å›ç­”:")
            print(response.response)
            
            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            if hasattr(response, 'source_nodes') and response.source_nodes:
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, node in enumerate(response.source_nodes, 1):
                    file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                    score = getattr(node, 'score', 0)
                    print(f"  {i}. {file_name} (ç›¸å…³åº¦: {score:.2f})")
                    
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
