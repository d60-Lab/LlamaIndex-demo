"""
çº¯æœ¬åœ° RAG åº”ç”¨ç¤ºä¾‹
ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€å¤–éƒ¨ API
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def main():
    """çº¯æœ¬åœ° RAG åº”ç”¨ä¸»å‡½æ•°"""
    
    print("ğŸš€ çº¯æœ¬åœ° RAG åº”ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹...")
    
    try:
        # LLM æ¨¡å‹
        llm = Ollama(
            model="deepseek-r1",
            base_url="http://localhost:11434",
            temperature=0.1,
            request_timeout=120.0
        )
        print(f"âœ… LLM æ¨¡å‹: {llm.model}")
        
        # åµŒå…¥æ¨¡å‹
        embed_model = OllamaEmbedding(
            model_name="nomic-embed-text",
            base_url="http://localhost:11434"
        )
        print(f"âœ… åµŒå…¥æ¨¡å‹: {embed_model.model_name}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")
        return
    
    # 2. åŠ è½½æ–‡æ¡£
    print("\nğŸ“ åŠ è½½æ–‡æ¡£æ•°æ®...")
    try:
        documents = SimpleDirectoryReader("./data").load_data()
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    except Exception as e:
        print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. æ–‡æ¡£åˆ†å—ï¼ˆä½¿ç”¨è¾ƒå°çš„å—å¤§å°ï¼‰
    print("\nğŸ”§ å¤„ç†æ–‡æ¡£åˆ†å—...")
    parser = SentenceSplitter(
        chunk_size=256,        # è¾ƒå°çš„å—å¤§å°
        chunk_overlap=25,      # é‡å éƒ¨åˆ†
        paragraph_separator="\n\n"
    )
    
    try:
        nodes = parser.get_nodes_from_documents(documents)
        print(f"âœ… æ–‡æ¡£è¢«åˆ†å‰²ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        print(f"âŒ æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
        return
    
    # 4. æ„å»ºç´¢å¼•
    print("\nğŸ—ï¸ æ„å»ºå‘é‡ç´¢å¼•...")
    try:
        index = VectorStoreIndex(
            nodes=nodes,
            embed_model=embed_model
        )
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
    except Exception as e:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•ä½¿ç”¨æ›´å°çš„æ–‡æ¡£é›†æˆ–æ£€æŸ¥åµŒå…¥æ¨¡å‹")
        return
    
    # 5. åˆ›å»ºæŸ¥è¯¢å¼•æ“
    print("\nğŸ” åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“...")
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        response_mode="compact",
        llm=llm,
    )
    
    # 6. äº¤äº’æŸ¥è¯¢
    print("\nğŸš€ æœ¬åœ° RAG ç³»ç»Ÿå·²å°±ç»ªï¼å¼€å§‹æé—®å§ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
    print("=" * 60)
    
    # é¢„è®¾ä¸€äº›æµ‹è¯•é—®é¢˜
    test_questions = [
        "LlamaIndex çš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ– RAG ç³»ç»Ÿçš„æ£€ç´¢æ€§èƒ½ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ ReAct ä»£ç†ï¼Ÿ"
    ]
    
    print("ğŸ’¡ ä½ å¯ä»¥å°è¯•ä»¥ä¸‹é—®é¢˜:")
    for i, q in enumerate(test_questions, 1):
        print(f"  {i}. {q}")
    print()
    
    while True:
        try:
            question = input("â“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not question:
                continue
            
            # å¦‚æœç”¨æˆ·è¾“å…¥æ•°å­—ï¼Œä½¿ç”¨é¢„è®¾é—®é¢˜
            if question.isdigit() and 1 <= int(question) <= len(test_questions):
                question = test_questions[int(question) - 1]
                print(f"ğŸ¯ ä½¿ç”¨é¢„è®¾é—®é¢˜: {question}")
            
            print("ğŸ¤” æ€è€ƒä¸­...")
            start_time = os.times()[4]
            
            response = query_engine.query(question)
            
            end_time = os.times()[4]
            query_time = end_time - start_time
            
            print(f"\nğŸ’¡ å›ç­”:")
            print(response.response)
            
            print(f"\nâ±ï¸ ç”¨æ—¶: {query_time:.2f}ç§’")
            
            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            if hasattr(response, 'source_nodes') and response.source_nodes:
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, node in enumerate(response.source_nodes, 1):
                    file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                    score = getattr(node, 'score', 0)
                    snippet = node.text[:100] + "..." if len(node.text) > 100 else node.text
                    print(f"  {i}. {file_name} (ç›¸å…³åº¦: {score:.2f})")
                    print(f"     ç‰‡æ®µ: {snippet}")
                    
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºç°é”™è¯¯: {e}")
            print("ğŸ’¡ è¯·æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")

if __name__ == "__main__":
    main()
