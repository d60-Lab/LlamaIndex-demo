# LlamaIndexï¼šä¼ä¸šçº§ LLM åº”ç”¨å¼€å‘æ¡†æ¶

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
- [LlamaAgents æ™ºèƒ½ä»£ç†](#llamaagents-æ™ºèƒ½ä»£ç†)
- [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [ç”Ÿæ€ç³»ç»Ÿ](#ç”Ÿæ€ç³»ç»Ÿ)
- [å®æˆ˜åœºæ™¯](#å®æˆ˜åœºæ™¯)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)

---

## æ¦‚è¿°

LlamaIndexï¼ˆåŸå GPT Indexï¼‰æ˜¯ä¸€ä¸ªä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡çš„å¼€æº LLM åº”ç”¨å¼€å‘æ¡†æ¶ã€‚å®ƒè§£å†³äº†æ„å»º RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨å’Œ AI ä»£ç†ç³»ç»Ÿä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š**å¦‚ä½•è®© LLM é«˜æ•ˆåœ°è®¿é—®å’Œç†è§£ç§æœ‰æ•°æ®**ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹© LlamaIndexï¼Ÿ

- ğŸš€ **å¼€ç®±å³ç”¨**ï¼š5 è¡Œä»£ç å³å¯æ„å»ºåŸºç¡€ RAG åº”ç”¨
- ğŸ”Œ **ä¸°å¯Œé›†æˆ**ï¼šæ”¯æŒ 160+ æ•°æ®æºå’Œ 50+ LLM æä¾›å•†
- ğŸ—ï¸ **ç”Ÿäº§å°±ç»ª**ï¼šå®Œæ•´çš„å¯è§‚æµ‹æ€§ã€è¯„ä¼°å’Œéƒ¨ç½²å·¥å…·é“¾
- ğŸ¤– **æ™ºèƒ½ä»£ç†**ï¼šå†…ç½® ReActã€Function Calling ç­‰ä»£ç†æ¨¡å¼
- ğŸ“ˆ **å¯æ‰©å±•æ€§**ï¼šä»åŸå‹åˆ°ç™¾ä¸‡çº§æ–‡æ¡£ç´¢å¼•çš„å¹³æ»‘æ‰©å±•

---

## æ ¸å¿ƒæ¦‚å¿µ

### RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å·¥ä½œæµ

```
ç”¨æˆ·æŸ¥è¯¢ â†’ æ•°æ®æ£€ç´¢ â†’ ä¸Šä¸‹æ–‡å¢å¼º â†’ LLM ç”Ÿæˆ â†’ ç»“æ„åŒ–è¾“å‡º
```

LlamaIndex åœ¨æ¯ä¸ªç¯èŠ‚éƒ½æä¾›äº†ä¼˜åŒ–å·¥å…·ï¼š

1. **æ•°æ®æ‘„å–**ï¼šç»Ÿä¸€æ¥å£å¤„ç†éç»“æ„åŒ–æ•°æ®
2. **ç´¢å¼•æ„å»º**ï¼šå¤šç§ç´¢å¼•ç­–ç•¥é€‚é…ä¸åŒåœºæ™¯
3. **æ™ºèƒ½æ£€ç´¢**ï¼šæ··åˆæ£€ç´¢ã€é‡æ’åºã€æŸ¥è¯¢å˜æ¢
4. **ç”Ÿæˆä¼˜åŒ–**ï¼šæç¤ºå·¥ç¨‹ã€å“åº”åˆæˆã€æµå¼è¾“å‡º

---

## æ¶æ„è®¾è®¡

### ä¸‰å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         åº”ç”¨å±‚ (Application Layer)       â”‚
â”‚  Query Engines | Agents | Chat Engines  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        ç´¢å¼•å±‚ (Indexing Layer)          â”‚
â”‚  Vector | Graph | Tree | Summary Index  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       æ•°æ®å±‚ (Data Layer)                â”‚
â”‚  Readers | Parsers | Transformers       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. æ•°æ®è¿æ¥å±‚

**æ”¯æŒçš„æ•°æ®æº**ï¼ˆ160+ ç§ï¼‰ï¼š
- ğŸ“„ **æ–‡æ¡£**ï¼šPDF, Word, Markdown, HTML
- ğŸ—„ï¸ **æ•°æ®åº“**ï¼šPostgreSQL, MongoDB, MySQL, Redis
- â˜ï¸ **äº‘æœåŠ¡**ï¼šGoogle Drive, Notion, Slack, Confluence
- ğŸŒ **API**ï¼šWeb scraping, RSS feeds, REST APIs
- ğŸ“Š **ç»“æ„åŒ–æ•°æ®**ï¼šCSV, Excel, JSON, Parquet

**å…³é”®ç‰¹æ€§**ï¼š
```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.notion import NotionPageReader

# ç»Ÿä¸€æ¥å£åŠ è½½ä¸åŒæ•°æ®æº
documents = SimpleDirectoryReader("./docs").load_data()
notion_docs = NotionPageReader(token="xxx").load_data()

# è‡ªåŠ¨æ–‡æ¡£è§£æå’Œå…ƒæ•°æ®æå–
for doc in documents:
    print(f"File: {doc.metadata['file_name']}")
    print(f"Size: {len(doc.text)} chars")
```

### 2. ç´¢å¼•ç³»ç»Ÿ

#### å‘é‡ç´¢å¼•ï¼ˆVector Store Indexï¼‰
æœ€å¸¸ç”¨çš„ç´¢å¼•ç±»å‹ï¼Œé€‚åˆè¯­ä¹‰æœç´¢ï¼š

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(
    documents,
    embed_model="local:BAAI/bge-small-en-v1.5"  # å¯é…ç½®åµŒå…¥æ¨¡å‹
)
```

#### æ ‘å½¢ç´¢å¼•ï¼ˆTree Indexï¼‰
å±‚çº§åŒ–ç»„ç»‡ï¼Œé€‚åˆå¤§è§„æ¨¡æ–‡æ¡£æ‘˜è¦ï¼š

```python
from llama_index.core import TreeIndex

tree_index = TreeIndex.from_documents(documents)
# è‡ªåŠ¨æ„å»ºæ‘˜è¦æ ‘ï¼Œæ”¯æŒè‡ªé¡¶å‘ä¸‹æŸ¥è¯¢
```

#### çŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ˆKnowledge Graph Indexï¼‰
æå–å®ä½“å…³ç³»ï¼Œæ”¯æŒç»“æ„åŒ–æ¨ç†ï¼š

```python
from llama_index.core import KnowledgeGraphIndex

kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=5,
    include_embeddings=True
)
```

#### åˆ—è¡¨ç´¢å¼•ï¼ˆList Indexï¼‰
ç®€å•é¡ºåºéå†ï¼Œé€‚åˆå°æ•°æ®é›†ï¼š

```python
from llama_index.core import SummaryIndex

list_index = SummaryIndex.from_documents(documents)
```

### 3. æŸ¥è¯¢å¼•æ“

**æ£€ç´¢ç­–ç•¥**ï¼š

```python
# 1. åŸºç¡€å‘é‡æ£€ç´¢
query_engine = index.as_query_engine(similarity_top_k=5)

# 2. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
)
query_engine = RetrieverQueryEngine(retriever=retriever)

# 3. å¸¦é‡æ’åºçš„æ£€ç´¢
from llama_index.postprocessor.cohere_rerank import CohereRerank

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[CohereRerank(top_n=3)]
)
```

**å“åº”åˆæˆæ¨¡å¼**ï¼š

```python
# compact: åˆå¹¶ä¸Šä¸‹æ–‡åä¸€æ¬¡æ€§ç”Ÿæˆ
# refine: è¿­ä»£å¼ç”Ÿæˆï¼Œé€æ­¥ç»†åŒ–ç­”æ¡ˆ
# tree_summarize: æ ‘å½¢èšåˆå¤šä¸ªå“åº”
# simple_summarize: æˆªæ–­ä¸Šä¸‹æ–‡åç”Ÿæˆ

query_engine = index.as_query_engine(
    response_mode="compact",
    streaming=True  # æµå¼è¾“å‡º
)
```

---

## æ ¸å¿ƒåŠŸèƒ½

### æ•°æ®åˆ†å—ï¼ˆChunkingï¼‰

åˆç†çš„åˆ†å—ç­–ç•¥ç›´æ¥å½±å“æ£€ç´¢è´¨é‡ï¼š

```python
from llama_index.core.node_parser import SentenceSplitter

# 1. åŸºäºå¥å­çš„åˆ†å—
splitter = SentenceSplitter(
    chunk_size=1024,        # token æ•°é‡
    chunk_overlap=200,      # é‡å éƒ¨åˆ†ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
    paragraph_separator="\n\n"
)

# 2. åŸºäºè¯­ä¹‰çš„åˆ†å—
from llama_index.core.node_parser import SemanticSplitterNodeParser

semantic_splitter = SemanticSplitterNodeParser(
    buffer_size=1,
    breakpoint_percentile_threshold=95
)

# 3. ä¿æŒä»£ç ç»“æ„çš„åˆ†å—
from llama_index.core.node_parser import CodeSplitter

code_splitter = CodeSplitter(
    language="python",
    chunk_lines=40,
    chunk_overlap_lines=5
)
```

### å…ƒæ•°æ®è¿‡æ»¤

ç²¾ç¡®æ§åˆ¶æ£€ç´¢èŒƒå›´ï¼š

```python
from llama_index.core.vector_stores import MetadataFilters, FilterCondition

filters = MetadataFilters(
    filters=[
        {"key": "category", "value": "technical"},
        {"key": "date", "value": "2024", "operator": ">="}
    ],
    condition=FilterCondition.AND
)

query_engine = index.as_query_engine(filters=filters)
```

### æŸ¥è¯¢å˜æ¢

ä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ä»¥æå‡æ£€ç´¢æ•ˆæœï¼š

```python
from llama_index.core.indices.query.query_transform import HyDEQueryTransform

# HyDE: ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£å†æ£€ç´¢
hyde = HyDEQueryTransform(include_original=True)
query_engine = index.as_query_engine(query_transform=hyde)

# å¤šæŸ¥è¯¢ç”Ÿæˆ
from llama_index.core.indices.query.query_transform import MultiQueryTransform
multi_query = MultiQueryTransform(num_queries=3)
```

---

## LlamaAgents æ™ºèƒ½ä»£ç†

### ä»£ç†æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **ReAct** | æ¨ç†-è¡ŒåŠ¨å¾ªç¯ | éœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ |
| **Function Calling** | ç»“æ„åŒ–å·¥å…·è°ƒç”¨ | API é›†æˆã€æ•°æ®åº“æ“ä½œ |
| **OpenAI Agents** | åŸç”Ÿå‡½æ•°è°ƒç”¨ | OpenAI æ¨¡å‹ä¸“ç”¨ |
| **LLMCompiler** | å¹¶è¡Œä»»åŠ¡æ‰§è¡Œ | éœ€è¦é«˜æ•ˆç‡çš„å¤æ‚å·¥ä½œæµ |

### ReAct ä»£ç†å®ç°

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool, QueryEngineTool
from llama_index.llms.openai import OpenAI

# 1. å®šä¹‰å·¥å…·å‡½æ•°
def multiply(a: float, b: float) -> float:
    """ä¸¤ä¸ªæ•°ç›¸ä¹˜"""
    return a * b

def search_docs(query: str) -> str:
    """æœç´¢æ–‡æ¡£åº“"""
    return query_engine.query(query).response

# 2. åˆ›å»ºå·¥å…·
multiply_tool = FunctionTool.from_defaults(fn=multiply)
search_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine,
    name="document_search",
    description="æœç´¢å…¬å¸å†…éƒ¨æ–‡æ¡£ï¼Œå›ç­”ä¸šåŠ¡ç›¸å…³é—®é¢˜"
)

# 3. åˆå§‹åŒ–ä»£ç†
agent = ReActAgent.from_tools(
    tools=[multiply_tool, search_tool],
    llm=OpenAI(model="gpt-4-turbo"),
    verbose=True,
    max_iterations=10
)

# 4. æ‰§è¡Œä»»åŠ¡
response = agent.chat(
    "æŸ¥æ‰¾æˆ‘ä»¬ Q3 çš„é”€å”®é¢ï¼Œç„¶åå°†å…¶ä¹˜ä»¥ 1.15 é¢„æµ‹ Q4 å¢é•¿"
)
```

### å¤šä»£ç†åä½œ

```python
from llama_index.core.agent import AgentRunner
from llama_index.core.workflow import Workflow

# å®šä¹‰ä¸“é—¨çš„ä»£ç†
research_agent = ReActAgent.from_tools([search_tool], llm=llm)
analysis_agent = ReActAgent.from_tools([calculator_tool], llm=llm)
writer_agent = ReActAgent.from_tools([document_tool], llm=llm)

# å·¥ä½œæµç¼–æ’
class ReportWorkflow(Workflow):
    async def run(self, topic: str):
        # ç ”ç©¶é˜¶æ®µ
        research = await research_agent.achat(f"ç ”ç©¶ä¸»é¢˜: {topic}")
        
        # åˆ†æé˜¶æ®µ
        analysis = await analysis_agent.achat(
            f"åˆ†æä»¥ä¸‹æ•°æ®: {research.response}"
        )
        
        # æ’°å†™é˜¶æ®µ
        report = await writer_agent.achat(
            f"åŸºäºä»¥ä¸‹åˆ†ææ’°å†™æŠ¥å‘Š: {analysis.response}"
        )
        
        return report.response

workflow = ReportWorkflow()
result = await workflow.run("2024å¹´AIè¡Œä¸šè¶‹åŠ¿")
```

### è®°å¿†ç³»ç»Ÿ

```python
from llama_index.core.memory import ChatMemoryBuffer

# çŸ­æœŸè®°å¿†ï¼ˆä¼šè¯çº§åˆ«ï¼‰
memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

agent = ReActAgent.from_tools(
    tools=tools,
    memory=memory,
    llm=llm
)

# é•¿æœŸè®°å¿†ï¼ˆæŒä¹…åŒ–ï¼‰
from llama_index.core.storage.chat_store import SimpleChatStore

chat_store = SimpleChatStore()
memory = ChatMemoryBuffer.from_defaults(
    token_limit=3000,
    chat_store=chat_store,
    chat_store_key="user_123"
)
```

---

## é«˜çº§ç‰¹æ€§

### 1. å¤šæ¨¡æ€å¤„ç†

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core import SimpleDirectoryReader

# åŠ è½½å›¾åƒæ–‡æ¡£
image_documents = SimpleDirectoryReader(
    input_files=["chart.png", "diagram.jpg"]
).load_data()

# å¤šæ¨¡æ€æŸ¥è¯¢
multimodal_llm = OpenAIMultiModal(model="gpt-4-vision-preview")
response = multimodal_llm.complete(
    prompt="åˆ†æè¿™å¼ é”€å”®å›¾è¡¨çš„è¶‹åŠ¿",
    image_documents=image_documents
)
```

### 2. ç»“æ„åŒ–è¾“å‡ºï¼ˆStructured Outputï¼‰

```python
from pydantic import BaseModel, Field
from llama_index.program.openai import OpenAIPydanticProgram

class CompanyInfo(BaseModel):
    """å…¬å¸ä¿¡æ¯"""
    name: str = Field(description="å…¬å¸åç§°")
    founded_year: int = Field(description="æˆç«‹å¹´ä»½")
    industry: str = Field(description="æ‰€å±è¡Œä¸š")
    revenue: float = Field(description="å¹´æ”¶å…¥ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰")
    employees: int = Field(description="å‘˜å·¥æ•°é‡")

# åˆ›å»ºæå–ç¨‹åº
program = OpenAIPydanticProgram.from_defaults(
    output_cls=CompanyInfo,
    prompt_template_str=(
        "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…¬å¸ä¿¡æ¯ï¼š\n"
        "{text}\n"
        "è¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®ã€‚"
    ),
    verbose=True
)

# æ‰§è¡Œæå–
company_info = program(text=document.text)
print(f"å…¬å¸: {company_info.name}")
print(f"æ”¶å…¥: ${company_info.revenue}M")
```

### 3. å­é—®é¢˜æŸ¥è¯¢ï¼ˆSub Question Queryï¼‰

å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼š

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool

# ä¸ºä¸åŒæ•°æ®æºåˆ›å»ºæŸ¥è¯¢å¼•æ“
sales_engine = sales_index.as_query_engine()
marketing_engine = marketing_index.as_query_engine()
product_engine = product_index.as_query_engine()

# åŒ…è£…ä¸ºå·¥å…·
tools = [
    QueryEngineTool.from_defaults(
        query_engine=sales_engine,
        name="sales_data",
        description="åŒ…å«é”€å”®æ•°æ®å’Œä¸šç»©æŒ‡æ ‡"
    ),
    QueryEngineTool.from_defaults(
        query_engine=marketing_engine,
        name="marketing_data",
        description="åŒ…å«è¥é”€æ´»åŠ¨å’ŒROIæ•°æ®"
    ),
    QueryEngineTool.from_defaults(
        query_engine=product_engine,
        name="product_data",
        description="åŒ…å«äº§å“ç‰¹æ€§å’Œç”¨æˆ·åé¦ˆ"
    )
]

# åˆ›å»ºå­é—®é¢˜æŸ¥è¯¢å¼•æ“
query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=tools,
    verbose=True
)

# è‡ªåŠ¨åˆ†è§£å¤æ‚æŸ¥è¯¢
response = query_engine.query(
    "å¯¹æ¯”Q1å’ŒQ2çš„é”€å”®è¡¨ç°ï¼Œåˆ†æè¥é”€æ´»åŠ¨çš„æ•ˆæœï¼Œ"
    "å¹¶è¯„ä¼°ç”¨æˆ·å¯¹æ–°äº§å“çš„åé¦ˆ"
)
```

### 4. è·¯ç”±æŸ¥è¯¢ï¼ˆRouter Queryï¼‰

æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€é€‰æ‹©æœ€ä½³æŸ¥è¯¢å¼•æ“ï¼š

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

# åˆ›å»ºè·¯ç”±å™¨
query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[
        QueryEngineTool.from_defaults(
            query_engine=vector_engine,
            description="ç”¨äºè¯­ä¹‰æœç´¢å’Œæ¦‚å¿µç†è§£"
        ),
        QueryEngineTool.from_defaults(
            query_engine=sql_engine,
            description="ç”¨äºç»“æ„åŒ–æ•°æ®æŸ¥è¯¢å’Œç»Ÿè®¡åˆ†æ"
        ),
        QueryEngineTool.from_defaults(
            query_engine=graph_engine,
            description="ç”¨äºå…³ç³»æ¨ç†å’Œå®ä½“å…³è”"
        )
    ]
)
```

### 5. è¯„ä¼°æ¡†æ¶

```python
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator
)

# å®šä¹‰è¯„ä¼°å™¨
faithfulness = FaithfulnessEvaluator(llm=llm)
relevancy = RelevancyEvaluator(llm=llm)
correctness = CorrectnessEvaluator(llm=llm)

# è¯„ä¼°æŸ¥è¯¢ç»“æœ
response = query_engine.query("LlamaIndexçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ")

faith_result = faithfulness.evaluate_response(response=response)
rel_result = relevancy.evaluate_response(
    query="LlamaIndexçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
    response=response
)

print(f"å¿ å®åº¦åˆ†æ•°: {faith_result.score}")
print(f"ç›¸å…³æ€§åˆ†æ•°: {rel_result.score}")
```

### 6. æ‰¹é‡è¯„ä¼°

```python
from llama_index.core.evaluation import BatchEvalRunner

# å‡†å¤‡æµ‹è¯•é›†
questions = [
    "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    "LlamaIndexæ”¯æŒå“ªäº›æ•°æ®æºï¼Ÿ",
    "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½ï¼Ÿ"
]

# æ‰¹é‡è¯„ä¼°
runner = BatchEvalRunner(
    {"faithfulness": faithfulness, "relevancy": relevancy},
    workers=8
)

eval_results = await runner.aevaluate_queries(
    query_engine=query_engine,
    queries=questions
)

# ç”ŸæˆæŠ¥å‘Š
print(eval_results)
```

---

## æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ

```python
from llama_index.core import Document
from llama_index.core.schema import TextNode

# âœ… æ¨èï¼šæ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
documents = [
    Document(
        text=content,
        metadata={
            "source": "internal_wiki",
            "department": "engineering",
            "last_updated": "2024-01-15",
            "author": "john@example.com",
            "version": "2.0",
            "tags": ["api", "authentication"]
        }
    )
]

# âœ… æ¨èï¼šè‡ªå®šä¹‰æ–‡æ¡£IDä¾¿äºæ›´æ–°
documents = [
    Document(
        text=content,
        id_="doc_123",  # è‡ªå®šä¹‰ID
        metadata=metadata
    )
]

# å¢é‡æ›´æ–°
index.refresh_ref_docs(documents)  # åªæ›´æ–°å˜åŒ–çš„æ–‡æ¡£
```

### 2. åˆ†å—ç­–ç•¥é€‰æ‹©

```python
# åœºæ™¯1ï¼šæŠ€æœ¯æ–‡æ¡£ - ä¿æŒä»£ç å®Œæ•´æ€§
from llama_index.core.node_parser import CodeSplitter
parser = CodeSplitter(language="python", chunk_lines=50)

# åœºæ™¯2ï¼šå¯¹è¯è®°å½• - ä¿æŒå¯¹è¯å®Œæ•´
from llama_index.core.node_parser import SentenceSplitter
parser = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separator="\n\n"  # æŒ‰å¯¹è¯åˆ†éš”
)

# åœºæ™¯3ï¼šå­¦æœ¯è®ºæ–‡ - è¯­ä¹‰åˆ†å—
from llama_index.core.node_parser import SemanticSplitterNodeParser
parser = SemanticSplitterNodeParser(
    embed_model=embed_model,
    breakpoint_percentile_threshold=95
)
```

### 3. ç´¢å¼•ä¼˜åŒ–

```python
# âœ… ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨é¿å…é‡å¤æ„å»º
from llama_index.core import StorageContext, load_index_from_storage

# é¦–æ¬¡æ„å»º
index = VectorStoreIndex.from_documents(documents)
index.storage_context.persist(persist_dir="./storage")

# åç»­åŠ è½½
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)

# âœ… å¼‚æ­¥æ„å»ºæå‡æ€§èƒ½
index = await VectorStoreIndex.afrom_documents(documents)

# âœ… æ‰¹é‡æ’å…¥ä¼˜åŒ–
index.insert_nodes(nodes, show_progress=True)
```

### 4. æ£€ç´¢ä¼˜åŒ–

```python
# âœ… æ··åˆæ£€ç´¢ç­–ç•¥
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever

vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=10)
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)

# èåˆæ£€ç´¢ç»“æœ
retriever = QueryFusionRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    similarity_top_k=5,
    mode="relative_score"  # ç›¸å¯¹åˆ†æ•°èåˆ
)

# âœ… é‡æ’åºæå‡ç²¾åº¦
from llama_index.postprocessor.cohere_rerank import CohereRerank

reranker = CohereRerank(api_key="xxx", top_n=3)
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker]
)
```

### 5. æˆæœ¬ä¼˜åŒ–

```python
# âœ… ä½¿ç”¨ç¼“å­˜å‡å°‘APIè°ƒç”¨
from llama_index.core import set_global_handler

set_global_handler("simple")  # å¯ç”¨ç¼“å­˜

# âœ… é€‰æ‹©æ€§ä½¿ç”¨å¼ºæ¨¡å‹
from llama_index.llms.openai import OpenAI

# æ£€ç´¢å’Œåˆæ­¥å¤„ç†ç”¨å°æ¨¡å‹
cheap_llm = OpenAI(model="gpt-3.5-turbo")
index = VectorStoreIndex.from_documents(documents, llm=cheap_llm)

# æœ€ç»ˆç”Ÿæˆç”¨å¼ºæ¨¡å‹
expensive_llm = OpenAI(model="gpt-4-turbo")
query_engine = index.as_query_engine(llm=expensive_llm)

# âœ… æœ¬åœ°åµŒå…¥æ¨¡å‹
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)
```

### 6. å¯è§‚æµ‹æ€§é…ç½®

```python
# âœ… é›†æˆ LangSmith è¿½è¸ª
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "xxx"

from llama_index.core import set_global_handler
set_global_handler("langsmith")

# âœ… è‡ªå®šä¹‰å›è°ƒ
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler

llama_debug = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([llama_debug])

# åº”ç”¨åˆ°ç´¢å¼•
index = VectorStoreIndex.from_documents(
    documents,
    callback_manager=callback_manager
)

# âœ… æ€§èƒ½ç›‘æ§
from llama_index.core.callbacks import TokenCountingHandler

token_counter = TokenCountingHandler()
callback_manager = CallbackManager([token_counter])

# æŸ¥è¯¢åæ£€æŸ¥tokenä½¿ç”¨
print(f"Embedding Tokens: {token_counter.total_embedding_token_count}")
print(f"LLM Prompt Tokens: {token_counter.prompt_llm_token_count}")
print(f"LLM Completion Tokens: {token_counter.completion_llm_token_count}")
```

---

## ç”Ÿæ€ç³»ç»Ÿ

### LLM æä¾›å•†æ”¯æŒ

| æä¾›å•† | æ¨¡å‹ç¤ºä¾‹ | å®‰è£…åŒ… |
|--------|----------|--------|
| OpenAI | GPT-4, GPT-3.5 | `llama-index-llms-openai` |
| Anthropic | Claude 3 | `llama-index-llms-anthropic` |
| Google | Gemini | `llama-index-llms-gemini` |
| Cohere | Command | `llama-index-llms-cohere` |
| Azure OpenAI | GPT-4 | `llama-index-llms-azure-openai` |
| æœ¬åœ°æ¨¡å‹ | Llama 2, Mistral | `llama-index-llms-ollama` |

### å‘é‡æ•°æ®åº“é›†æˆ

| æ•°æ®åº“ | ç‰¹ç‚¹ | é€‚ç”¨è§„æ¨¡ |
|--------|------|----------|
| Pinecone | æ‰˜ç®¡æœåŠ¡ï¼Œæ˜“ç”¨ | ä¸­å¤§å‹ |
| Weaviate | å¼€æºï¼ŒåŠŸèƒ½ä¸°å¯Œ | å¤§å‹ |
| Chroma | è½»é‡çº§ï¼Œæœ¬åœ°ä¼˜å…ˆ | å°ä¸­å‹ |
| Milvus | é«˜æ€§èƒ½ï¼Œå¯æ‰©å±• | å¤§å‹ |
| Qdrant | Rustå®ç°ï¼Œå¿«é€Ÿ | ä¸­å¤§å‹ |
| FAISS | Metaå¼€æºï¼Œå†…å­˜å‹ | å°ä¸­å‹ |

```python
# Pinecone ç¤ºä¾‹
from llama_index.vector_stores.pinecone import PineconeVectorStore
import pinecone

pinecone.init(api_key="xxx", environment="us-west1-gcp")
vector_store = PineconeVectorStore(pinecone_index=index)

# Chroma ç¤ºä¾‹
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

chroma_client = chromadb.Client()
vector_store = ChromaVectorStore(chroma_collection=collection)
```

### æ¡†æ¶é›†æˆ

```python
# ä¸ LangChain äº’æ“ä½œ
from langchain.agents import Tool
from llama_index.langchain_helpers.agents import LlamaToolkit

toolkit = LlamaToolkit(index=index)
tools = toolkit.get_tools()

# ä¸ FastAPI é›†æˆ
from fastapi import FastAPI
app = FastAPI()

@app.post("/query")
async def query(question: str):
    response = await query_engine.aquery(question)
    return {"answer": response.response}

# ä¸ Streamlit é›†æˆ
import streamlit as st

st.title("ä¼ä¸šçŸ¥è¯†åº“é—®ç­”")
question = st.text_input("è¯·è¾“å…¥é—®é¢˜ï¼š")
if question:
    with st.spinner("æ€è€ƒä¸­..."):
        response = query_engine.query(question)
        st.write(response.response)
```

---

## å®æˆ˜åœºæ™¯

### åœºæ™¯ 1ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# 1. æ•°æ®åŠ è½½
documents = SimpleDirectoryReader(
    input_dir="./company_docs",
    recursive=True,
    required_exts=[".pdf", ".docx", ".md"]
).load_data()

# 2. æ–‡æ¡£å¤„ç†
parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
nodes = parser.get_nodes_from_documents(documents)

# 3. æ„å»ºç´¢å¼•
index = VectorStoreIndex(
    nodes=nodes,
    embed_model=OpenAIEmbedding(model="text-embedding-3-small")
)

# 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆå¸¦å¼•ç”¨ï¼‰
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact",
    llm=OpenAI(model="gpt-4"),
)

# 5. æŸ¥è¯¢å¹¶æ˜¾ç¤ºæ¥æº
response = query_engine.query("å…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"ç­”æ¡ˆ: {response.response}")
print("\næ¥æºæ–‡æ¡£:")
for node in response.source_nodes:
    print(f"- {node.metadata['file_name']} (ç›¸å…³åº¦: {node.score:.2f})")
```

### åœºæ™¯ 2ï¼šSQL æ•°æ®åº“æ™ºèƒ½æŸ¥è¯¢

```python
from llama_index.core import SQLDatabase
from llama_index.core.query_engine import NLSQLTableQueryEngine
from sqlalchemy import create_engine

# 1. è¿æ¥æ•°æ®åº“
engine = create_engine("postgresql://user:pass@localhost/sales_db")
sql_database = SQLDatabase(engine, include_tables=["orders", "customers"])

# 2. åˆ›å»ºNL2SQLæŸ¥è¯¢å¼•æ“
query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=["orders", "customers"],
    llm=OpenAI(model="gpt-4")
)

# 3. è‡ªç„¶è¯­è¨€æŸ¥è¯¢
response = query_engine.query(
    "åˆ—å‡º2024å¹´Q1é”€å”®é¢è¶…è¿‡10ä¸‡çš„å®¢æˆ·ï¼ŒæŒ‰é”€å”®é¢é™åºæ’åˆ—"
)
print(response.response)
print(f"\nç”Ÿæˆçš„SQL: {response.metadata['sql_query']}")
```

### åœºæ™¯ 3ï¼šå¤šæ–‡æ¡£å¯¹æ¯”åˆ†æ

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine import SubQuestionQueryEngine

# 1. ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºç´¢å¼•
doc_2023 = SimpleDirectoryReader(input_files=["report_2023.pdf"]).load_data()
doc_2024 = SimpleDirectoryReader(input_files=["report_2024.pdf"]).load_data()

index_2023 = VectorStoreIndex.from_documents(doc_2023)
index_2024 = VectorStoreIndex.from_documents(doc_2024)

# 2. åˆ›å»ºæŸ¥è¯¢å·¥å…·
tools = [
    QueryEngineTool.from_defaults(
        query_engine=index_2023.as_query_engine(),
        name="report_2023",
        description="2023å¹´åº¦ä¸šåŠ¡æŠ¥å‘Š"
    ),
    QueryEngineTool.from_defaults(
        query_engine=index_2024.as_query_engine(),
        name="report_2024",
        description="2024å¹´åº¦ä¸šåŠ¡æŠ¥å‘Š"
    )
]

# 3. å­é—®é¢˜å¼•æ“è‡ªåŠ¨åˆ†è§£å¯¹æ¯”é—®é¢˜
query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=tools,
    verbose=True
)

response = query_engine.query(
    "å¯¹æ¯”2023å’Œ2024å¹´çš„è¥æ”¶å¢é•¿ç‡å’Œåˆ©æ¶¦ç‡ï¼Œåˆ†æå˜åŒ–åŸå› "
)
```

### åœºæ™¯ 4ï¼šå®¢æœæœºå™¨äººï¼ˆå¸¦å·¥å•ç³»ç»Ÿï¼‰

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool

# 1. å®šä¹‰å·¥å…·å‡½æ•°
def search_knowledge_base(query: str) -> str:
    """æœç´¢çŸ¥è¯†åº“å›ç­”å¸¸è§é—®é¢˜"""
    response = kb_query_engine.query(query)
    return response.response

def create_ticket(issue: str, priority: str) -> str:
    """åˆ›å»ºå·¥å•"""
    ticket_id = tickets_api.create(issue=issue, priority=priority)
    return f"å·²åˆ›å»ºå·¥å• #{ticket_id}ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å¤„ç†"

def check_order_status(order_id: str) -> str:
    """æŸ¥è¯¢è®¢å•çŠ¶æ€"""
    status = orders_db.get_status(order_id)
    return f"è®¢å• {order_id} çŠ¶æ€: {status}"

# 2. åˆ›å»ºå·¥å…·
tools = [
    FunctionTool.from_defaults(fn=search_knowledge_base),
    FunctionTool.from_defaults(fn=create_ticket),
    FunctionTool.from_defaults(fn=check_order_status)
]

# 3. åˆå§‹åŒ–å®¢æœä»£ç†
agent = ReActAgent.from_tools(
    tools=tools,
    llm=OpenAI(model="gpt-4"),
    system_prompt=(
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœä»£ç†ã€‚"
        "ä¼˜å…ˆä»çŸ¥è¯†åº“æŸ¥æ‰¾ç­”æ¡ˆï¼Œå¦‚æœæ— æ³•è§£å†³åˆ™åˆ›å»ºå·¥å•ã€‚"
        "å§‹ç»ˆä¿æŒç¤¼è²Œå’Œä¸“ä¸šã€‚"
    ),
    max_iterations=5
)

# 4. å¤„ç†ç”¨æˆ·è¯·æ±‚
response = agent.chat("æˆ‘çš„è®¢å• #12345 ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ")
```

### åœºæ™¯ 5ï¼šä»£ç åº“é—®ç­”åŠ©æ‰‹

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import CodeSplitter
from llama_index.readers.file import FlatReader

# 1. åŠ è½½ä»£ç æ–‡ä»¶
reader = FlatReader()
documents = reader.load_data(
    input_dir="./src",
    include_exts=[".py", ".js", ".ts"]
)

# 2. ä½¿ç”¨ä»£ç åˆ†å—å™¨
splitter = CodeSplitter(
    language="python",
    chunk_lines=100,
    chunk_overlap_lines=10,
    max_chars=2000
)
nodes = splitter.get_nodes_from_documents(documents)

# 3. æ·»åŠ å‡½æ•°çº§å…ƒæ•°æ®
for node in nodes:
    # æå–å‡½æ•°åã€ç±»åç­‰
    code = node.text
    if "def " in code:
        func_name = code.split("def ")[1].split("(")[0]
        node.metadata["function_name"] = func_name

# 4. æ„å»ºç´¢å¼•
index = VectorStoreIndex(nodes=nodes)

# 5. æŸ¥è¯¢
query_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="compact"
)

response = query_engine.query(
    "å¦‚ä½•å®ç°ç”¨æˆ·è®¤è¯ï¼Ÿè¯·ç»™å‡ºç›¸å…³ä»£ç ç¤ºä¾‹"
)
```

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install llama-index

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«æ‰€æœ‰é›†æˆï¼‰
pip install llama-index[all]

# æŒ‰éœ€å®‰è£…ç‰¹å®šé›†æˆ
pip install llama-index-llms-anthropic
pip install llama-index-vector-stores-pinecone
pip install llama-index-readers-notion
```

### 5 åˆ†é’Ÿå¿«é€Ÿå…¥é—¨

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1. åŠ è½½æ•°æ®ï¼ˆå°†æ–‡æ¡£æ”¾åœ¨ ./data ç›®å½•ï¼‰
documents = SimpleDirectoryReader("data").load_data()

# 2. æ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# 3. æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")

print(response)
```

### é…ç½®ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx
COHERE_API_KEY=xxx
PINECONE_API_KEY=xxx
PINECONE_ENVIRONMENT=us-west1-gcp
```

```python
# åœ¨ä»£ç ä¸­åŠ è½½
from dotenv import load_dotenv
load_dotenv()
```

### æŒä¹…åŒ–ç¤ºä¾‹

```python
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage
)
import os

PERSIST_DIR = "./storage"

# é¦–æ¬¡è¿è¡Œï¼šæ„å»ºå¹¶ä¿å­˜ç´¢å¼•
if not os.path.exists(PERSIST_DIR):
    documents = SimpleDirectoryReader("data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir=PERSIST_DIR)
else:
    # åç»­è¿è¡Œï¼šåŠ è½½å·²æœ‰ç´¢å¼•
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)

query_engine = index.as_query_engine()
```

---

## å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- ğŸ“š [å®Œæ•´æ–‡æ¡£](https://docs.llamaindex.ai/)
- ğŸ“ [æ•™ç¨‹ä¸­å¿ƒ](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)
- ğŸ’¡ [ç¤ºä¾‹ä»£ç åº“](https://github.com/run-llama/llama_index/tree/main/docs/examples)

### ç¤¾åŒºèµ„æº
- ğŸ’¬ [Discord ç¤¾åŒº](https://discord.gg/dGcwcsnxhU)
- ğŸ™ [GitHub ä»“åº“](https://github.com/run-llama/llama_index)
- ğŸ¥ [YouTube æ•™ç¨‹](https://www.youtube.com/@LlamaIndex)

### æ¨èå­¦ä¹ è·¯å¾„
1. **åˆçº§**ï¼šå®˜æ–¹å¿«é€Ÿå…¥é—¨ â†’ åŸºç¡€RAGåº”ç”¨
2. **ä¸­çº§**ï¼šæŸ¥è¯¢å¼•æ“ä¼˜åŒ– â†’ å¤šç§ç´¢å¼•ç­–ç•¥
3. **é«˜çº§**ï¼šæ™ºèƒ½ä»£ç†å¼€å‘ â†’ å¤šä»£ç†åä½œç³»ç»Ÿ
4. **ä¸“å®¶**ï¼šç”Ÿäº§éƒ¨ç½² â†’ æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

---

## æ€»ç»“

LlamaIndex é€šè¿‡å…¶**æ¨¡å—åŒ–æ¶æ„**å’Œ**ä¸°å¯Œçš„å·¥å…·é“¾**ï¼Œå·²æˆä¸ºæ„å»ºç”Ÿäº§çº§ LLM åº”ç”¨çš„é¦–é€‰æ¡†æ¶ã€‚å®ƒç‰¹åˆ«é€‚åˆï¼š

âœ… éœ€è¦å¤„ç†**å¤§é‡ç§æœ‰æ•°æ®**çš„ä¼ä¸šåº”ç”¨  
âœ… éœ€è¦**å¤æ‚æ¨ç†å’Œå·¥å…·è°ƒç”¨**çš„ AI ä»£ç†  
âœ… éœ€è¦**é«˜åº¦å®šåˆ¶åŒ–**çš„ RAG ç³»ç»Ÿ  
âœ… éœ€è¦**å¿«é€ŸåŸå‹åˆ°ç”Ÿäº§éƒ¨ç½²**çš„é¡¹ç›®

é€šè¿‡åˆç†è¿ç”¨å…¶ç´¢å¼•ç­–ç•¥ã€æ£€ç´¢ä¼˜åŒ–ã€ä»£ç†ç³»ç»Ÿå’Œè¯„ä¼°æ¡†æ¶ï¼Œå¼€å‘è€…å¯ä»¥æ„å»ºå‡ºé«˜è´¨é‡ã€å¯æ‰©å±•çš„æ™ºèƒ½åº”ç”¨ã€‚
